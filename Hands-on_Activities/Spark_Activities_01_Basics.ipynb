{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 1: Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# provide path to your spark directory directly\n",
    "findspark.init(\"~/spark2\")\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ok, so we have an error. Now what?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Did you start the Spark instance first?**\n",
    "\n",
    "        cd spark2/sbin\n",
    "        ./start-master.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Have you specifed the path correctly?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# provide path to your spark directory directly\n",
    "findspark.init(\"/Users/soumendra/spark2/\")\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's create a SparkContext and use it to count the number of lines in a file. For that, let's create a text file first.**\n",
    "\n",
    "        cd\n",
    "        ls >> helloworld\n",
    "        cat helloworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"helloworld\")\n",
    "\n",
    "# let's test our setup by counting the number of nonempty lines in a text file\n",
    "lines = sc.textFile('/Users/soumendra/helloworld')\n",
    "lines_nonempty = lines.filter( lambda x: len(x) > 0 )\n",
    "lines_nonempty.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ok, so we can't run multiple SparkContexts at once! What about running the one created before?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's test our setup by counting the number of nonempty lines in a text file\n",
    "lines = sc.textFile('/Users/soumendra/helloworld')\n",
    "lines_nonempty = lines.filter( lambda x: len(x) > 0 )\n",
    "lines_nonempty.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 2: Using Anonyous Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's use *lambda* to create an anonymous function to count the number of lines containing *Python*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "text=\"Python is a fun language,\\nbut then what language\\nis not, if\\nI may ask. But Python\\nis also.\"\n",
    "\n",
    "echo -e $text > python.txt\n",
    "cat python.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"/Users/soumendra/python.txt\")\n",
    "pythonLines = lines.filter(lambda line: \"Python\" in line)\n",
    "print(\"No of lines containing 'Python':\", pythonLines.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Well, do explain the answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 3: Counting Primes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weâ€™ll go ahead and calculate the number of primes less than a given large number. To start with, we'll define a function that determines the primality of any given number (we'll later parallelize this function on a set of numbers).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isprime(n):\n",
    "    \"\"\"\n",
    "    check if integer n is a prime\n",
    "    \"\"\"\n",
    "    # make sure n is a positive integer\n",
    "    n = abs(int(n))\n",
    "    # 0 and 1 are not primes\n",
    "    if n < 2:\n",
    "        return False\n",
    "    # 2 is the only even prime number\n",
    "    if n == 2:\n",
    "        return True\n",
    "    # all other even numbers are not primes\n",
    "    if not n & 1:\n",
    "        return False\n",
    "    # range starts with 3 and only needs to go up the square root of n\n",
    "    # for all odd numbers\n",
    "    for x in range(3, int(n**0.5)+1, 2):\n",
    "        if n % x == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an RDD of numbers from 0 to 1,000,000\n",
    "nums = sc.parallelize(range(1000000))\n",
    "\n",
    "# Compute the number of primes in the RDD\n",
    "print(nums.filter(isprime).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 4:  Word and Line Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from operator import add\n",
    "\n",
    "filein = sc.textFile('/Users/soumendra/helloworld')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count lines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('number of lines in file: %s' % filein.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count non-empty lines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filein_nonempty = filein.filter( lambda x: len(x) > 0 )\n",
    "print('number of non-empty lines in file: %s' % filein_nonempty.count()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count no of characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars = filein.map(lambda s: len(s)).reduce(add)\n",
    "print('number of characters in file: %s' % chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count words of length greater than 3 characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = filein.flatMap(lambda line: re.split('\\W+', line.lower().strip()))\n",
    "words = words.filter(lambda x: len(x) > 3)\n",
    "\n",
    "words = words.map(lambda w: (w,1))\n",
    "words = words.reduceByKey(add)\n",
    "print('number of words with more than 3 characters in file: %s' % words.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 5: Workflow Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-91adbf884ec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## Imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m## Module Constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "## Spark Application Template - execute with spark-submit\n",
    "\n",
    "## Imports\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "## Module Constants\n",
    "APP_NAME = \"Name of Application\"  #helps in debugging\n",
    "\n",
    "## Closure Functions\n",
    "\n",
    "## Main functionality\n",
    "\n",
    "def main(sc):\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure Spark\n",
    "    conf = SparkConf().setAppName(APP_NAME)\n",
    "    conf = conf.setMaster(\"local[*]\")\n",
    "    sc   = SparkContext(conf=conf)\n",
    "\n",
    "    # Execute Main functionality\n",
    "    main(sc)\n",
    "\n",
    "# To close or exit the program use sc.stop() or sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from add import *\n",
    "\n",
    "add(5,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 6: Sample Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "# provide path to your spark directory directly\n",
    "findspark.init(\"/Users/soumendra/spark2/\")\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "## Imports\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from operator import add, itemgetter\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "## Module Constants\n",
    "APP_NAME = \"Flight Delay Analysis\"\n",
    "DATE_FMT = \"%Y-%m-%d\"\n",
    "TIME_FMT = \"%H%M\"\n",
    "\n",
    "fields   = ('date', 'airline', 'flightnum', 'origin', 'dest', 'dep',\n",
    "            'dep_delay', 'arv', 'arv_delay', 'airtime', 'distance')\n",
    "Flight   = namedtuple('Flight', fields)\n",
    "\n",
    "## Closure Functions\n",
    "def parse(row):\n",
    "    \"\"\"\n",
    "    Parses a row and returns a named tuple.\n",
    "    \"\"\"\n",
    "\n",
    "    row[0]  = datetime.strptime(row[0], DATE_FMT).date()\n",
    "    row[5]  = datetime.strptime(row[5], TIME_FMT).time()\n",
    "    row[6]  = float(row[6])\n",
    "    row[7]  = datetime.strptime(row[7], TIME_FMT).time()\n",
    "    row[8]  = float(row[8])\n",
    "    row[9]  = float(row[9])\n",
    "    row[10] = float(row[10])\n",
    "    return Flight(*row[:11])\n",
    "\n",
    "def split(line):\n",
    "    \"\"\"\n",
    "    Operator function for splitting a line with csv module\n",
    "    \"\"\"\n",
    "    reader = csv.reader(StringIO(line))\n",
    "    return reader.next()\n",
    "\n",
    "def plot(delays):\n",
    "    \"\"\"\n",
    "    Show a bar chart of the total delay per airline\n",
    "    \"\"\"\n",
    "    airlines = [d[0] for d in delays]\n",
    "    minutes  = [d[1] for d in delays]\n",
    "    index    = list(xrange(len(airlines)))\n",
    "\n",
    "    fig, axe = plt.subplots()\n",
    "    bars = axe.barh(index, minutes)\n",
    "\n",
    "    # Add the total minutes to the right\n",
    "    for idx, air, min in zip(index, airlines, minutes):\n",
    "        if min > 0:\n",
    "            bars[idx].set_color('#d9230f')\n",
    "            axe.annotate(\" %0.0f min\" % min, xy=(min+1, idx+0.5), va='center')\n",
    "        else:\n",
    "            bars[idx].set_color('#469408')\n",
    "            axe.annotate(\" %0.0f min\" % min, xy=(10, idx+0.5), va='center')\n",
    "\n",
    "    # Set the ticks\n",
    "    ticks = plt.yticks([idx+ 0.5 for idx in index], airlines)\n",
    "    xt = plt.xticks()[0]\n",
    "    plt.xticks(xt, [' '] * len(xt))\n",
    "\n",
    "    # minimize chart junk\n",
    "    plt.grid(axis = 'x', color ='white', linestyle='-')\n",
    "\n",
    "    plt.title('Total Minutes Delayed per Airline')\n",
    "    plt.show()\n",
    "\n",
    "## Main functionality\n",
    "def main(sc):\n",
    "\n",
    "    # Load the airlines lookup dictionary\n",
    "    airlines = dict(sc.textFile(\"ontime/airlines.csv\").map(split).collect())\n",
    "\n",
    "    # Broadcast the lookup dictionary to the cluster\n",
    "    airline_lookup = sc.broadcast(airlines)\n",
    "\n",
    "    # Read the CSV Data into an RDD\n",
    "    flights = sc.textFile(\"ontime/flights.csv\").map(split).map(parse)\n",
    "\n",
    "    # Map the total delay to the airline (joined using the broadcast value)\n",
    "    delays  = flights.map(lambda f: (airline_lookup.value[f.airline],\n",
    "                                     add(f.dep_delay, f.arv_delay)))\n",
    "\n",
    "    # Reduce the total delay for the month to the airline\n",
    "    delays  = delays.reduceByKey(add).collect()\n",
    "    delays  = sorted(delays, key=itemgetter(1))\n",
    "\n",
    "    # Provide output from the driver\n",
    "    for d in delays:\n",
    "        print(\"%0.0f minutes delayed\\t%s\" % (d[1], d[0]))\n",
    "\n",
    "    # Show a bar chart of the delays\n",
    "    plot(delays)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure Spark\n",
    "    conf = SparkConf().setMaster(\"local[*]\")\n",
    "    conf = conf.setAppName(APP_NAME)\n",
    "    sc   = SparkContext(conf=conf)\n",
    "    # Uncomment the lines above when running the application with \"submit\" (spark-submit app.py)\n",
    "    # Comment the lines above out when running in IPython Notebook\n",
    "\n",
    "    # Execute Main functionality\n",
    "    main(sc)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
